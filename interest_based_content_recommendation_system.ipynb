{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0d967434",
      "metadata": {
        "id": "0d967434"
      },
      "source": [
        "# Interest-based Content Recommendation System\n",
        "This notebook contains the patched implementation of Hybrid Recommender with:\n",
        "1. Separate vectorizers for users and posts.\n",
        "2. Global MinMaxScaler fitted once on training scores, reused during inference.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install scikit-surprise"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RU3O_bY5ypvD",
        "outputId": "06bd5814-253f-4da0-e957-9a477c759b82"
      },
      "id": "RU3O_bY5ypvD",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-surprise in /usr/local/lib/python3.12/dist-packages (1.1.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-surprise) (1.5.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.12/dist-packages (from scikit-surprise) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-surprise) (1.16.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install numpy==1.26.4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W3f6nVX7y77-",
        "outputId": "86cde0da-31c6-410b-b08a-babc1d6b0db3"
      },
      "id": "W3f6nVX7y77-",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.12/dist-packages (1.26.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "fb9cfd41",
      "metadata": {
        "id": "fb9cfd41"
      },
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import roc_auc_score, ndcg_score\n",
        "from surprise import Dataset, Reader, SVD\n",
        "from collections import defaultdict\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "aabd2b21",
      "metadata": {
        "id": "aabd2b21"
      },
      "outputs": [],
      "source": [
        "\n",
        "class HybridRecommender:\n",
        "    def __init__(self, content_weight=0.3):\n",
        "        self.content_model = lgb.LGBMClassifier(objective='binary', random_state=42, verbose=-1)\n",
        "        self.collab_model = SVD(n_factors=150, random_state=42)\n",
        "        self.content_weight = content_weight\n",
        "        self.collab_weight = 1 - content_weight\n",
        "\n",
        "        # Encoders & scalers\n",
        "        self.user_encoder = LabelEncoder()\n",
        "        self.post_encoder = LabelEncoder()\n",
        "        self.user_vectorizer = TfidfVectorizer()\n",
        "        self.post_vectorizer = TfidfVectorizer()\n",
        "        self.age_scaler = StandardScaler()\n",
        "        self.popularity_scaler = StandardScaler()\n",
        "        self.content_score_scaler = MinMaxScaler()\n",
        "        self.collab_score_scaler = MinMaxScaler()\n",
        "\n",
        "        self.user_features = None\n",
        "        self.post_features = None\n",
        "\n",
        "        self.users_df = None\n",
        "        self.posts_df = None\n",
        "        self.all_engagements = None\n",
        "        self.val_users = None\n",
        "        self.val_true_engagements = defaultdict(list)\n",
        "\n",
        "    def _prepare_features(self):\n",
        "        print(\"Preparing features for the content-based model...\")\n",
        "        users, posts = self.users_df, self.posts_df\n",
        "\n",
        "        # User features\n",
        "        users['gender_encoded'] = LabelEncoder().fit_transform(users['gender'])\n",
        "        users['age_scaled'] = self.age_scaler.fit_transform(users[['age']])\n",
        "        user_interests = self.user_vectorizer.fit_transform(users['top_3_interests'])\n",
        "        self.user_features = np.hstack([\n",
        "            users[['gender_encoded', 'age_scaled', 'past_engagement_score']].values,\n",
        "            user_interests.toarray()\n",
        "        ])\n",
        "\n",
        "        # Post features\n",
        "        posts['content_type_encoded'] = LabelEncoder().fit_transform(posts['content_type'])\n",
        "        post_tags = self.post_vectorizer.fit_transform(posts['tags'])\n",
        "        posts['popularity_scaled'] = self.popularity_scaler.fit_transform(posts[['popularity_score']])\n",
        "\n",
        "        self.post_features = np.hstack([\n",
        "            posts[['content_type_encoded', 'popularity_scaled']].values,\n",
        "            post_tags.toarray()\n",
        "        ])\n",
        "\n",
        "    def fit(self, users, posts, engagements):\n",
        "        self.users_df = users.copy()\n",
        "        self.posts_df = posts.copy()\n",
        "        self.all_engagements = engagements.copy()\n",
        "\n",
        "        print(\"Calculating post popularity feature...\")\n",
        "        popularity_df = engagements.groupby('post_id').size().reset_index(name='popularity_score')\n",
        "        self.posts_df = pd.merge(self.posts_df, popularity_df, on='post_id', how='left')\n",
        "        self.posts_df['popularity_score'].fillna(0, inplace=True)\n",
        "\n",
        "        self.users_df['user_idx'] = self.user_encoder.fit_transform(self.users_df['user_id'])\n",
        "        self.posts_df['post_idx'] = self.post_encoder.fit_transform(self.posts_df['post_id'])\n",
        "\n",
        "        self._prepare_features()\n",
        "\n",
        "        print(\"Splitting data into training and validation sets...\")\n",
        "        unique_users = users['user_id'].unique()\n",
        "        train_users, self.val_users = train_test_split(unique_users, test_size=0.2, random_state=42)\n",
        "\n",
        "        train_engagements = engagements[engagements['user_id'].isin(train_users)]\n",
        "        val_engagements = engagements[engagements['user_id'].isin(self.val_users)]\n",
        "\n",
        "        for _, row in val_engagements.iterrows():\n",
        "            if row['engagement'] == 1:\n",
        "                self.val_true_engagements[row['user_id']].append(row['post_id'])\n",
        "\n",
        "        train_data = pd.merge(train_engagements, self.users_df, on='user_id')\n",
        "        train_data = pd.merge(train_data, self.posts_df, on='post_id')\n",
        "\n",
        "        X = np.hstack([self.user_features[train_data['user_idx'].values], self.post_features[train_data['post_idx'].values]])\n",
        "        y = train_data['engagement']\n",
        "\n",
        "        print(\"Training content-based model (LightGBM)...\")\n",
        "        self.content_model.fit(X, y)\n",
        "\n",
        "        print(\"Training collaborative filtering model (SVD)...\")\n",
        "        reader = Reader(rating_scale=(0, 1))\n",
        "        train_data_surprise = Dataset.load_from_df(train_engagements[['user_id', 'post_id', 'engagement']], reader)\n",
        "        trainset = train_data_surprise.build_full_trainset()\n",
        "        self.collab_model.fit(trainset)\n",
        "\n",
        "        # --- Fit global scalers on training predictions ---\n",
        "        print(\"Fitting global score scalers...\")\n",
        "        content_scores = self.content_model.predict_proba(X)[:, 1]\n",
        "        collab_scores = [self.collab_model.predict(uid, iid).est for uid, iid in zip(train_data['user_id'], train_data['post_id'])]\n",
        "        self.content_score_scaler.fit(np.array(content_scores).reshape(-1, 1))\n",
        "        self.collab_score_scaler.fit(np.array(collab_scores).reshape(-1, 1))\n",
        "\n",
        "        print(\"Training complete!\")\n",
        "\n",
        "    def _get_blended_scores(self, user_id, post_ids):\n",
        "        post_df_subset = self.posts_df[self.posts_df['post_id'].isin(post_ids)].copy()\n",
        "        if post_df_subset.empty: return pd.DataFrame(columns=['post_id', 'final_score'])\n",
        "\n",
        "        user_idx = self.users_df.loc[self.users_df['user_id'] == user_id, 'user_idx'].iloc[0]\n",
        "        user_vec = self.user_features[user_idx]\n",
        "        post_indices = post_df_subset['post_idx'].values\n",
        "\n",
        "        user_vec_repeated = np.tile(user_vec, (len(post_indices), 1))\n",
        "        feature_matrix = np.hstack([user_vec_repeated, self.post_features[post_indices]])\n",
        "\n",
        "        content_scores = self.content_model.predict_proba(feature_matrix)[:, 1]\n",
        "        collab_scores = [self.collab_model.predict(user_id, post_id).est for post_id in post_df_subset['post_id']]\n",
        "\n",
        "        norm_content = self.content_score_scaler.transform(np.array(content_scores).reshape(-1, 1)).flatten()\n",
        "        norm_collab = self.collab_score_scaler.transform(np.array(collab_scores).reshape(-1, 1)).flatten()\n",
        "\n",
        "        post_df_subset['final_score'] = (self.content_weight * norm_content) + (self.collab_weight * norm_collab)\n",
        "\n",
        "        return post_df_subset[['post_id', 'final_score']]\n",
        "\n",
        "    def recommend(self, user_id, k=3):\n",
        "        seen_posts = set(self.all_engagements[self.all_engagements['user_id'] == user_id]['post_id'])\n",
        "        candidate_post_ids = self.posts_df[~self.posts_df['post_id'].isin(seen_posts)]['post_id'].tolist()\n",
        "        if not candidate_post_ids: return []\n",
        "        scores_df = self._get_blended_scores(user_id, candidate_post_ids)\n",
        "        top_k_posts = scores_df.sort_values('final_score', ascending=False).head(k)\n",
        "        return top_k_posts['post_id'].tolist()\n",
        "\n",
        "    def evaluate(self, n_neg_samples=100):\n",
        "        print(\"Evaluating model performance on validation users...\")\n",
        "        all_precisions, all_recalls, all_aucs, all_ndcgs = [], [], [], []\n",
        "        all_post_ids = self.posts_df['post_id'].unique()\n",
        "\n",
        "        for user_id in self.val_users:\n",
        "            true_positives = self.val_true_engagements.get(user_id, [])\n",
        "            if not true_positives: continue\n",
        "\n",
        "            recommendations = self.recommend(user_id, k=3)\n",
        "            hits = len(set(recommendations) & set(true_positives))\n",
        "            all_precisions.append(hits / len(recommendations) if recommendations else 0)\n",
        "            all_recalls.append(hits / len(true_positives))\n",
        "\n",
        "            seen_posts = set(self.all_engagements[self.all_engagements['user_id'] == user_id]['post_id'])\n",
        "            possible_negatives = list(set(all_post_ids) - seen_posts - set(true_positives))\n",
        "            neg_samples = np.random.choice(possible_negatives, size=min(len(possible_negatives), n_neg_samples), replace=False)\n",
        "            eval_items = true_positives + list(neg_samples)\n",
        "            scores_df = self._get_blended_scores(user_id, eval_items).set_index('post_id')\n",
        "\n",
        "            y_true = np.array([1 if item in true_positives else 0 for item in scores_df.index])\n",
        "            y_score = scores_df['final_score'].values\n",
        "\n",
        "            if len(np.unique(y_true)) > 1:\n",
        "                all_aucs.append(roc_auc_score(y_true, y_score))\n",
        "            all_ndcgs.append(ndcg_score([y_true], [y_score], k=10))\n",
        "\n",
        "        avg_precision = np.mean(all_precisions) if all_precisions else 0\n",
        "        avg_recall = np.mean(all_recalls) if all_recalls else 0\n",
        "        avg_auc = np.mean(all_aucs) if all_aucs else 0\n",
        "        avg_ndcg = np.mean(all_ndcgs) if all_ndcgs else 0\n",
        "\n",
        "        print(f\"  -> Average Precision@3: {avg_precision:.4f}\")\n",
        "        print(f\"  -> Average Recall@3:    {avg_recall:.4f}\")\n",
        "        print(f\"  -> Average AUC:         {avg_auc:.4f}\")\n",
        "        print(f\"  -> Average nDCG@10:     {avg_ndcg:.4f}\")\n",
        "        return avg_precision, avg_recall, avg_auc, avg_ndcg\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "d3def894",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3def894",
        "outputId": "2c6f40b5-26c9-4560-e49f-9c708b99a594"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Testing with content_weight=0.1, n_factors=50 ---\n",
            "Calculating post popularity feature...\n",
            "Preparing features for the content-based model...\n",
            "Splitting data into training and validation sets...\n",
            "Training content-based model (LightGBM)...\n",
            "Training collaborative filtering model (SVD)...\n",
            "Fitting global score scalers...\n",
            "Training complete!\n",
            "Evaluating model performance on validation users...\n",
            "  -> Average Precision@3: 0.0000\n",
            "  -> Average Recall@3:    0.0000\n",
            "  -> Average AUC:         0.5258\n",
            "  -> Average nDCG@10:     0.0960\n",
            "\n",
            "--- Testing with content_weight=0.1, n_factors=150 ---\n",
            "Calculating post popularity feature...\n",
            "Preparing features for the content-based model...\n",
            "Splitting data into training and validation sets...\n",
            "Training content-based model (LightGBM)...\n",
            "Training collaborative filtering model (SVD)...\n",
            "Fitting global score scalers...\n",
            "Training complete!\n",
            "Evaluating model performance on validation users...\n",
            "  -> Average Precision@3: 0.0000\n",
            "  -> Average Recall@3:    0.0000\n",
            "  -> Average AUC:         0.5332\n",
            "  -> Average nDCG@10:     0.0892\n",
            "\n",
            "--- Testing with content_weight=0.1, n_factors=200 ---\n",
            "Calculating post popularity feature...\n",
            "Preparing features for the content-based model...\n",
            "Splitting data into training and validation sets...\n",
            "Training content-based model (LightGBM)...\n",
            "Training collaborative filtering model (SVD)...\n",
            "Fitting global score scalers...\n",
            "Training complete!\n",
            "Evaluating model performance on validation users...\n",
            "  -> Average Precision@3: 0.0000\n",
            "  -> Average Recall@3:    0.0000\n",
            "  -> Average AUC:         0.5282\n",
            "  -> Average nDCG@10:     0.1203\n",
            "\n",
            "--- Testing with content_weight=0.2, n_factors=50 ---\n",
            "Calculating post popularity feature...\n",
            "Preparing features for the content-based model...\n",
            "Splitting data into training and validation sets...\n",
            "Training content-based model (LightGBM)...\n",
            "Training collaborative filtering model (SVD)...\n",
            "Fitting global score scalers...\n",
            "Training complete!\n",
            "Evaluating model performance on validation users...\n",
            "  -> Average Precision@3: 0.0000\n",
            "  -> Average Recall@3:    0.0000\n",
            "  -> Average AUC:         0.5145\n",
            "  -> Average nDCG@10:     0.0791\n",
            "\n",
            "--- Testing with content_weight=0.2, n_factors=150 ---\n",
            "Calculating post popularity feature...\n",
            "Preparing features for the content-based model...\n",
            "Splitting data into training and validation sets...\n",
            "Training content-based model (LightGBM)...\n",
            "Training collaborative filtering model (SVD)...\n",
            "Fitting global score scalers...\n",
            "Training complete!\n",
            "Evaluating model performance on validation users...\n",
            "  -> Average Precision@3: 0.0000\n",
            "  -> Average Recall@3:    0.0000\n",
            "  -> Average AUC:         0.5200\n",
            "  -> Average nDCG@10:     0.0767\n",
            "\n",
            "--- Testing with content_weight=0.2, n_factors=200 ---\n",
            "Calculating post popularity feature...\n",
            "Preparing features for the content-based model...\n",
            "Splitting data into training and validation sets...\n",
            "Training content-based model (LightGBM)...\n",
            "Training collaborative filtering model (SVD)...\n",
            "Fitting global score scalers...\n",
            "Training complete!\n",
            "Evaluating model performance on validation users...\n",
            "  -> Average Precision@3: 0.0000\n",
            "  -> Average Recall@3:    0.0000\n",
            "  -> Average AUC:         0.5155\n",
            "  -> Average nDCG@10:     0.0980\n",
            "\n",
            "--- Testing with content_weight=0.3, n_factors=50 ---\n",
            "Calculating post popularity feature...\n",
            "Preparing features for the content-based model...\n",
            "Splitting data into training and validation sets...\n",
            "Training content-based model (LightGBM)...\n",
            "Training collaborative filtering model (SVD)...\n",
            "Fitting global score scalers...\n",
            "Training complete!\n",
            "Evaluating model performance on validation users...\n",
            "  -> Average Precision@3: 0.0000\n",
            "  -> Average Recall@3:    0.0000\n",
            "  -> Average AUC:         0.5038\n",
            "  -> Average nDCG@10:     0.0708\n",
            "\n",
            "--- Testing with content_weight=0.3, n_factors=150 ---\n",
            "Calculating post popularity feature...\n",
            "Preparing features for the content-based model...\n",
            "Splitting data into training and validation sets...\n",
            "Training content-based model (LightGBM)...\n",
            "Training collaborative filtering model (SVD)...\n",
            "Fitting global score scalers...\n",
            "Training complete!\n",
            "Evaluating model performance on validation users...\n",
            "  -> Average Precision@3: 0.0000\n",
            "  -> Average Recall@3:    0.0000\n",
            "  -> Average AUC:         0.5057\n",
            "  -> Average nDCG@10:     0.0674\n",
            "\n",
            "--- Testing with content_weight=0.3, n_factors=200 ---\n",
            "Calculating post popularity feature...\n",
            "Preparing features for the content-based model...\n",
            "Splitting data into training and validation sets...\n",
            "Training content-based model (LightGBM)...\n",
            "Training collaborative filtering model (SVD)...\n",
            "Fitting global score scalers...\n",
            "Training complete!\n",
            "Evaluating model performance on validation users...\n",
            "  -> Average Precision@3: 0.0000\n",
            "  -> Average Recall@3:    0.0000\n",
            "  -> Average AUC:         0.5031\n",
            "  -> Average nDCG@10:     0.0777\n",
            "\n",
            "==============================================\n",
            "Grid Search Complete!\n",
            "Best AUC Score: 0.5332\n",
            "Best Parameters: {'content_weight': 0.1, 'n_factors': 150}\n",
            "==============================================\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        users = pd.read_csv(\"Users.csv\")\n",
        "        posts = pd.read_csv(\"Posts.csv\")\n",
        "        engagements = pd.read_csv(\"Engagements.csv\")\n",
        "\n",
        "        users['top_3_interests'].fillna('', inplace=True)\n",
        "        posts['tags'].fillna('', inplace=True)\n",
        "\n",
        "        # --- Grid Search Setup ---\n",
        "        best_auc = 0\n",
        "        best_params = {}\n",
        "\n",
        "        # Define the grid of parameters to test\n",
        "        param_grid = {\n",
        "            'content_weight': [0.1, 0.2, 0.3],\n",
        "            'n_factors': [50, 150, 200]\n",
        "        }\n",
        "\n",
        "        # Loop through all combinations\n",
        "        for weight in param_grid['content_weight']:\n",
        "            for factors in param_grid['n_factors']:\n",
        "                print(f\"\\n--- Testing with content_weight={weight}, n_factors={factors} ---\")\n",
        "\n",
        "                # Initialize and train the model with the current parameters\n",
        "                recommender = HybridRecommender(content_weight=weight)\n",
        "                recommender.collab_model = SVD(n_factors=factors, random_state=42) # Set n_factors here\n",
        "\n",
        "                recommender.fit(users, posts, engagements)\n",
        "                _, _, avg_auc, _ = recommender.evaluate()\n",
        "\n",
        "                # Check if this is the best model so far\n",
        "                if avg_auc > best_auc:\n",
        "                    best_auc = avg_auc\n",
        "                    best_params = {'content_weight': weight, 'n_factors': factors}\n",
        "\n",
        "        print(\"\\n==============================================\")\n",
        "        print(f\"Grid Search Complete!\")\n",
        "        print(f\"Best AUC Score: {best_auc:.4f}\")\n",
        "        print(f\"Best Parameters: {best_params}\")\n",
        "        print(\"==============================================\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(\"Error: Make sure Users.csv, Posts.csv, and Engagements.csv are in the same directory.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Example run after training ---\n",
        "\n",
        "if recommender.val_users.size > 0:\n",
        "    sample_user_id = recommender.val_users[0]\n",
        "    print(f\"\\n--- Example Recommendations for User: {sample_user_id} ---\")\n",
        "\n",
        "    user_info = users[users['user_id'] == sample_user_id]\n",
        "    print(\n",
        "        f\"User Profile: Age={user_info['age'].iloc[0]}, \"\n",
        "        f\"Gender={user_info['gender'].iloc[0]}, \"\n",
        "        f\"Interests='{user_info['top_3_interests'].iloc[0]}'\"\n",
        "    )\n",
        "\n",
        "    top_3_recs = recommender.recommend(sample_user_id, k=3)\n",
        "\n",
        "    print(\"\\nTop 3 Recommended Post IDs:\")\n",
        "    if top_3_recs:\n",
        "        for i, post_id in enumerate(top_3_recs):\n",
        "            print(f\"{i+1}. {post_id}\")\n",
        "    else:\n",
        "        print(\"Could not generate recommendations for this user.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yu6FdZdFz_ii",
        "outputId": "7a5176bd-008f-44da-8f36-9929a7c98de2"
      },
      "id": "yu6FdZdFz_ii",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Example Recommendations for User: U14 ---\n",
            "User Profile: Age=19, Gender=M, Interests='sports, fitness, travel'\n",
            "\n",
            "Top 3 Recommended Post IDs:\n",
            "1. P88\n",
            "2. P46\n",
            "3. P41\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bfq-HRM_0Q_l"
      },
      "id": "bfq-HRM_0Q_l",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}